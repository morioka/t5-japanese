{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "t5-japanese-article-generation-inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae67849dc0834877b84009db13d6378e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a6d72fdc434441ba8ea22a48e956cc64",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c1c62444fbb4c3f950a9256fdf9f9ee",
              "IPY_MODEL_d4dc2cae599348eba79565dc39a53b0c"
            ]
          }
        },
        "a6d72fdc434441ba8ea22a48e956cc64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c1c62444fbb4c3f950a9256fdf9f9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d068187ef8a14ead8d22ceec668c19d5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7926212f4d9c4ff4b033e7f60eafc639"
          }
        },
        "d4dc2cae599348eba79565dc39a53b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8ff1556cd32e4e6ba12c9161525e1b30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 46/46 [16:24&lt;00:00, 21.40s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6348398f03cd4308a4ddb5153fce0c23"
          }
        },
        "d068187ef8a14ead8d22ceec668c19d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7926212f4d9c4ff4b033e7f60eafc639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ff1556cd32e4e6ba12c9161525e1b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6348398f03cd4308a4ddb5153fce0c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVvMN6EZZgeZ"
      },
      "source": [
        "# ニュース記事本文生成（推論のみ）\n",
        "\n",
        "転移学習済みT5モデルを用いて、ニュース記事本文生成を実行します。\n",
        "\n",
        "- **T5（Text-to-Text Transfer Transformer）**: テキストを入力されるとテキストを出力するという統一的枠組みで様々な自然言語処理タスクを解く深層学習モデル（[日本語解説](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part7.html)）  \n",
        "<img src=\"https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s1600/image2.png\">  \n",
        "出典: [Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
        "- **事前学習**: 個別のタスク用に学習をする前に文法や一般的な言葉の文脈的意味を学習させること（自己教師あり学習とWikipedia等の大規模データ（コーパス）を用いることで広く一般的な知識を持ったモデルを作れる）\n",
        "- **転移学習、ファインチューニング**: 事前学習済みモデルを初期値にして、特定のタスク用に追加で学習を行うこと（主に教師あり学習）\n",
        "\n",
        "入出力が次の形式を持ったタスク用に転移学習されたモデルを用いて推論を実行します。\n",
        "\n",
        "- **入力**: \"{title}\"をトークナイズしたトークンID列（最大64トークン）\n",
        "- **出力**: \"{body}\"をトークナイズしたトークンID列（最大512トークン）\n",
        "\n",
        "ここで、{title}はニュース記事のタイトル、{body}は本文、{genre_id}はニュースの分類ラベル（0〜8）です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96yzpdcmZlPL"
      },
      "source": [
        "# ライブラリやデータの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCMX3LJUZohT"
      },
      "source": [
        "## 依存ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JQI6sIAAR2T"
      },
      "source": [
        "!pip install -qU torch==1.7.1 torchtext==0.8.0 torchvision==0.8.2 torchaudio==0.7.2\n",
        "!pip install -q transformers==4.4.2 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKPw3bnyJPkP"
      },
      "source": [
        "## 各種ディレクトリ作成\n",
        "\n",
        "* data: テスト用データセット格納用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0e7X5jYZtKm"
      },
      "source": [
        "!mkdir -p /content/data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Jwcu4Fcgy9"
      },
      "source": [
        "# 転移学習済みモデル\n",
        "MODEL_DIR = \"sonoisa/t5-base-japanese-article-generation\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYnEg051Fl1v"
      },
      "source": [
        "## livedoor ニュースコーパスのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pga3WMu6FJYn"
      },
      "source": [
        "!wget -O ldcc-20140209.tar.gz https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfrLGFbSF7tn"
      },
      "source": [
        "## livedoorニュースコーパスの形式変換\n",
        "\n",
        "livedoorニュースコーパスを次の形式のTSVファイルに変換します。\n",
        "\n",
        "* 1列目: タイトル\n",
        "* 2列目: 本文\n",
        "* 3列目: ジャンルID（0〜8）\n",
        "\n",
        "TSVファイルは/content/dataに格納されます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sv7HD9AZ4h9"
      },
      "source": [
        "## 文字列の正規化の定義\n",
        "\n",
        "表記揺れを減らします。今回は[neologdの正規化処理](https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja)を一部改変したものを利用します。\n",
        "処理の詳細はリンク先を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-0eoBYFNKH"
      },
      "source": [
        "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    return s"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNcXZaboFsNV"
      },
      "source": [
        "## 情報抽出\n",
        "\n",
        "ニュース記事のタイトルと本文とジャンル（9分類）の情報を抽出します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGKW7gi2FOG1"
      },
      "source": [
        "import tarfile\n",
        "import re\n",
        "\n",
        "target_genres = [\"dokujo-tsushin\",\n",
        "                 \"it-life-hack\",\n",
        "                 \"kaden-channel\",\n",
        "                 \"livedoor-homme\",\n",
        "                 \"movie-enter\",\n",
        "                 \"peachy\",\n",
        "                 \"smax\",\n",
        "                 \"sports-watch\",\n",
        "                 \"topic-news\"]\n",
        "\n",
        "def remove_brackets(text):\n",
        "    text = re.sub(r\"(^【[^】]*】)|(【[^】]*】$)\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def normalize_text(text):\n",
        "    assert \"\\n\" not in text and \"\\r\" not in text\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = text.strip()\n",
        "    text = normalize_neologd(text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def read_title_body(file):\n",
        "    next(file)\n",
        "    next(file)\n",
        "    title = next(file).decode(\"utf-8\").strip()\n",
        "    title = normalize_text(remove_brackets(title))\n",
        "    body = normalize_text(\" \".join([line.decode(\"utf-8\").strip() for line in file.readlines()]))\n",
        "    return title, body\n",
        "\n",
        "genre_files_list = [[] for genre in target_genres]\n",
        "\n",
        "all_data = []\n",
        "\n",
        "with tarfile.open(\"ldcc-20140209.tar.gz\") as archive_file:\n",
        "    for archive_item in archive_file:\n",
        "        for i, genre in enumerate(target_genres):\n",
        "            if genre in archive_item.name and archive_item.name.endswith(\".txt\"):\n",
        "                genre_files_list[i].append(archive_item.name)\n",
        "\n",
        "    for i, genre_files in enumerate(genre_files_list):\n",
        "        for name in genre_files:\n",
        "            file = archive_file.extractfile(name)\n",
        "            title, body = read_title_body(file)\n",
        "            title = normalize_text(title)\n",
        "            body = normalize_text(body)\n",
        "\n",
        "            if len(title) > 0 and len(body) > 0:\n",
        "                all_data.append({\n",
        "                    \"title\": title,\n",
        "                    \"body\": body,\n",
        "                    \"genre_id\": i\n",
        "                    })"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jVIyipoG2Es"
      },
      "source": [
        "## データ分割\n",
        "\n",
        "データセットを90% : 5%: 5% の比率でtrain/dev/testに分割します。\n",
        "\n",
        "* trainデータ: 学習に利用するデータ\n",
        "* devデータ: 学習中の精度評価等に利用するデータ\n",
        "* testデータ: 学習結果のモデルの精度評価に利用するデータ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Fzhe5qFV3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d13ac83-376d-48e9-8292-d5f7a94f8f73"
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(1234)\n",
        "random.shuffle(all_data)\n",
        "\n",
        "def to_line(data):\n",
        "    title = data[\"title\"]\n",
        "    body = data[\"body\"]\n",
        "    genre_id = data[\"genre_id\"]\n",
        "\n",
        "    assert len(title) > 0 and len(body) > 0\n",
        "    return f\"{title}\\t{body}\\t{genre_id}\\n\"\n",
        "\n",
        "data_size = len(all_data)\n",
        "train_ratio, dev_ratio, test_ratio = 0.9, 0.05, 0.05\n",
        "\n",
        "with open(f\"data/train.tsv\", \"w\", encoding=\"utf-8\") as f_train, \\\n",
        "    open(f\"data/dev.tsv\", \"w\", encoding=\"utf-8\") as f_dev, \\\n",
        "    open(f\"data/test.tsv\", \"w\", encoding=\"utf-8\") as f_test:\n",
        "    \n",
        "    for i, data in tqdm(enumerate(all_data)):\n",
        "        line = to_line(data)\n",
        "        if i < train_ratio * data_size:\n",
        "            f_train.write(line)\n",
        "        elif i < (train_ratio + dev_ratio) * data_size:\n",
        "            f_dev.write(line)\n",
        "        else:\n",
        "            f_test.write(line)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7334it [00:00, 107545.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB79fsa9aCxs"
      },
      "source": [
        "作成されたデータを確認します。\n",
        "\n",
        "形式: {タイトル}\\t{本文}\\t{ジャンルID}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GstRhKSyFaK7"
      },
      "source": [
        "!head -3 data/test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP1hD944aFJp"
      },
      "source": [
        "# 推論に必要なクラス等の定義\n",
        "\n",
        "推論にはPyTorch/Transformersを利用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ZzooLKA-j5"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        ")\n",
        "\n",
        "# 乱数シードの設定\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1EyFckzBV_Y"
      },
      "source": [
        "# GPU利用有無\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "# 各種ハイパーパラメータ\n",
        "args_dict = dict(\n",
        "    data_dir=\"/content/data\",  # データセットのディレクトリ\n",
        "    model_name_or_path=MODEL_DIR,\n",
        "    tokenizer_name_or_path=MODEL_DIR,\n",
        "\n",
        "    max_input_length=64,\n",
        "    max_target_length=512,\n",
        "\n",
        "    n_gpu=1 if USE_GPU else 0,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False,\n",
        "    opt_level='O1',\n",
        "    max_grad_norm=1.0,\n",
        "    seed=42,\n",
        ")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kmjeVRaakCT"
      },
      "source": [
        "## TSVデータセットクラス\n",
        "\n",
        "TSV形式のファイルをデータセットとして読み込みます。  \n",
        "形式は\"{title}\\t{body}\\t{genre_id}\"です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEgMB3pjD6q_"
      },
      "source": [
        "class TsvDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512):\n",
        "        self.file_path = os.path.join(data_dir, type_path)\n",
        "        \n",
        "        self.input_max_len = input_max_len\n",
        "        self.target_max_len = target_max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        source_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \n",
        "                \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _make_record(self, title, body, genre_id):\n",
        "        # ニュースタイトル生成タスク用の入出力形式に変換する。\n",
        "        input = f\"{title}\"\n",
        "        target = f\"{body}\"\n",
        "        return input, target\n",
        "  \n",
        "    def _build(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip().split(\"\\t\")\n",
        "                assert len(line) == 3\n",
        "                assert len(line[0]) > 0\n",
        "                assert len(line[1]) > 0\n",
        "                assert len(line[2]) > 0\n",
        "\n",
        "                title = line[0]\n",
        "                body = line[1]\n",
        "                genre_id = line[2]\n",
        "\n",
        "                input, target = self._make_record(title, body, genre_id)\n",
        "\n",
        "                tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                    [input], max_length=self.input_max_len, truncation=True, \n",
        "                    padding=\"max_length\", return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                    [target], max_length=self.target_max_len, truncation=True, \n",
        "                    padding=\"max_length\", return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                self.inputs.append(tokenized_inputs)\n",
        "                self.targets.append(tokenized_targets)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WxUifqia09e"
      },
      "source": [
        "試しにテストデータ（test.tsv）を読み込み、トークナイズ結果をみてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-MdRkUSCD0m"
      },
      "source": [
        "# トークナイザー（SentencePiece）モデルの読み込み\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR, is_fast=True)\n",
        "\n",
        "# テストデータセットの読み込み\n",
        "train_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"train.tsv\", \n",
        "                           input_max_len=args_dict[\"max_input_length\"], \n",
        "                           target_max_len=args_dict[\"max_target_length\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DURizjcAbB3-"
      },
      "source": [
        "テストデータの1レコード目をみてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKKjKUi4IUdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076fdfde-99ea-467f-886d-7bd547f9737f"
      },
      "source": [
        "for data in train_dataset:\n",
        "    print(\"A. 入力データの元になる文字列\")\n",
        "    print(tokenizer.decode(data[\"source_ids\"]))\n",
        "    print()\n",
        "    print(\"B. 入力データ（Aの文字列がトークナイズされたトークンID列）\")\n",
        "    print(data[\"source_ids\"])\n",
        "    print()\n",
        "    print(\"C. 出力データの元になる文字列\")\n",
        "    print(tokenizer.decode(data[\"target_ids\"]))\n",
        "    print()\n",
        "    print(\"D. 出力データ（Cの文字列がトークナイズされたトークンID列）\")\n",
        "    print(data[\"target_ids\"])\n",
        "    break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A. 入力データの元になる文字列\n",
            "hulu、ついに待望のapple tvに対応!一周年記念で「huluアンバサダープロジェクト」もスタート</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "B. 入力データ（Aの文字列がトークナイズされたトークンID列）\n",
            "tensor([21105,  4763,     3,  5057,  3888,  1798,     6, 21868, 15594,  6477,\n",
            "          253,    82,  6838,    15,    19, 10654,  4763,   500,   206,   211,\n",
            "          626,  1318,    17,    28,  2330,     1,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n",
            "\n",
            "C. 出力データの元になる文字列\n",
            "huluは昨年9月にサービスを開始して以来、日本でのビジネスを順調に拡大してきた。現在は約1000本の映画と、10,000話以上に及ぶテレビ番組の視聴が可能だ。そんなhuluから新たなニュースが飛び込んできた。 ■今日からapple tvに対応!今日、9月4日から新たに「apple tv」にてhuluを楽しめるようになったという。aple idを用いて新規登録や決済も可能だという。apple tvのホーム画面からhuluを起動し、すでにアカウントを持っているユーザーはログイン。また、まだアカウントを持っていない人も簡単にitunesアカウントで登録できる。apple tvより新規登録したユーザーは通常2週間の無料トライアルが1ヶ月になるということだ。 ■サービス一周年記念「huluアンバサダープロジェクト」9月12日からサービス一周年を記念したスペシャルプロジェクト「huluアンバサダープロジェクト」が実施される。facebookやtwitterを通してhuluについて情報発信すると「huluアンバサダー」に任命され、先着10,000名の新規登録者は1ケ月無料トライアルが楽しめるということだ。コンテンツも対応デバイスもますます増えるhuluに今後も注目だ。</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "D. 出力データ（Cの文字列がトークナイズされたトークンID列）\n",
            "tensor([21105,  4763,     7, 11708,    12,    50,   133,   710,  4397, 20082,\n",
            "            3, 11666,  3259,    14, 15134,  1915,  2107,     4,  1190,   216,\n",
            "         1961,  2819,   246,    16,     3, 21366,   388,   790,  7279,  4983,\n",
            "            6,  9927,  4190,   317,     4,  9817, 10654,  4763,    24,  1454,\n",
            "         1428,     8,  4598,  3713,  2239,     4,     5, 26488,  4634,    24,\n",
            "        21868, 15594,  6477,   253,  4634,     3,    50,    18,    31,   573,\n",
            "         1922,    19, 21868, 15594,    17,   244, 10654,  4763,    14, 17181,\n",
            "         1176,    47,     4,    96, 12306,     5,  2204,  2383,  3706,  1313,\n",
            "           22, 19993, 11187,  6063,     4, 21868, 15594,     6,   814,  2811,\n",
            "           24, 10654,  4763,    14, 11020,    29,     3,  3082,  7155,  3520,\n",
            "         2441,     7, 25304,     4,   238,     3,  2289,  7155,  4317,  2069,\n",
            "        11549,  9322, 25522,  7155,    15,  1313,   348,     4, 21868, 15594,\n",
            "           89,  3706,  1313,    27,  2441,  6707,    20,  2603,     6,  4210,\n",
            "        17370,     8,    21,  2123,   365,  3320,   317,     4,     5, 26488,\n",
            "          710,    82,  6838,    19, 10654,  4763,   500,   206,   211,   626,\n",
            "         1318,    17,    50,    18,    63,   573,   710,    82, 30756,    38,\n",
            "         2701,  1318,    19, 10654,  4763,   500,   206,   211,   626,  1318,\n",
            "           17,     8, 19022,     4, 25219,    22, 13316,  3477, 10654,  4763,\n",
            "          240,   517, 10335,   412,    19, 10654,  4763,   500,   206,   211,\n",
            "          626,    17, 12430,     3,   544,   451, 21366,  2235,  3706,  1313,\n",
            "         1680,    21,   401,    18,  4210, 17370,     8, 17181,  3320,   317,\n",
            "            4,  6485,    28,  1246, 12126,    28, 20373,  8275,    55, 10654,\n",
            "         4763,    13,  5572,    28,  8049,   317,     4,     1,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3Rz4DCbdTt1"
      },
      "source": [
        "# 学習済みモデルの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCCzprs3Btb8"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# トークナイザー（SentencePiece）\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR, is_fast=True)\n",
        "\n",
        "# 学習済みモデル\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
        "\n",
        "# GPUの利用有無\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "if USE_GPU:\n",
        "    trained_model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk_RiWWfdjNp"
      },
      "source": [
        "# 全テストデータの本文に対するタイトル生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7qyYdvrEm-9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ae67849dc0834877b84009db13d6378e",
            "a6d72fdc434441ba8ea22a48e956cc64",
            "8c1c62444fbb4c3f950a9256fdf9f9ee",
            "d4dc2cae599348eba79565dc39a53b0c",
            "d068187ef8a14ead8d22ceec668c19d5",
            "7926212f4d9c4ff4b033e7f60eafc639",
            "8ff1556cd32e4e6ba12c9161525e1b30",
            "6348398f03cd4308a4ddb5153fce0c23"
          ]
        },
        "outputId": "da31bdd1-ed1d-435e-bd0c-dfd56702dedb"
      },
      "source": [
        "import textwrap\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"test.tsv\", \n",
        "                          input_max_len=args_dict[\"max_input_length\"], \n",
        "                          target_max_len=args_dict[\"max_target_length\"])\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=4)\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "targets = []\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids = batch['source_ids']\n",
        "    input_mask = batch['source_mask']\n",
        "    if USE_GPU:\n",
        "        input_ids = input_ids.cuda()\n",
        "        input_mask = input_mask.cuda()\n",
        "\n",
        "    output = trained_model.generate(input_ids=input_ids, \n",
        "        attention_mask=input_mask, \n",
        "        max_length=args_dict[\"max_target_length\"],\n",
        "        repetition_penalty=10.0,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n",
        "        )\n",
        "\n",
        "    output_text = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                            clean_up_tokenization_spaces=False) \n",
        "                for ids in output]\n",
        "    target_text = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                               clean_up_tokenization_spaces=False) \n",
        "                for ids in batch[\"target_ids\"]]\n",
        "    input_text = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                               clean_up_tokenization_spaces=False) \n",
        "                for ids in input_ids]\n",
        "\n",
        "    inputs.extend(input_text)\n",
        "    outputs.extend(output_text)\n",
        "    targets.extend(target_text)\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae67849dc0834877b84009db13d6378e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlZLyaLzenwG"
      },
      "source": [
        "## 生成結果確認\n",
        "\n",
        "形式\n",
        "- title: ニュース記事のタイトル\n",
        "- generated: 生成された本文\n",
        "- actual: 人が作成した本文（正解）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAl6qQhJFJA3"
      },
      "source": [
        "for output, target, input in zip(outputs, targets, inputs):\n",
        "    print(\"title:     \" + input)\n",
        "    print(\"generated: \" + output)\n",
        "    print(\"actual:    \" + target)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZyXm5rEkTHJ"
      },
      "source": [
        "# タイトルに合ったニュース記事本文を生成\n",
        "\n",
        "タイトルに合う記事を自動生成してみます。\n",
        "\n",
        "以下のコードでは生成される文章の変な繰り返しを防いだりするために色々generateメソッドのパラメータを設定しています。パラメータの詳細は下記リンク先を参照してください。\n",
        "\n",
        "- [generateメソッドのパラメータの意味](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZGe9inMkZZI"
      },
      "source": [
        "title = \"LEGOで作るスマートロック　〜「Hey Siri 鍵開けて」を実現する方法 〜\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hISkwsHkwpu",
        "outputId": "9ebca9eb-3228-4ad4-c3d7-f534c25b1937"
      },
      "source": [
        "MAX_SOURCE_LENGTH = args_dict[\"max_input_length\"]   # 入力される記事本文の最大トークン数\n",
        "MAX_TARGET_LENGTH = args_dict[\"max_target_length\"]  # 生成されるタイトルの最大トークン数\n",
        "\n",
        "def preprocess_title(text):\n",
        "    return normalize_text(text.replace(\"\\n\", \"\"))\n",
        "\n",
        "# 推論モード設定\n",
        "trained_model.eval()\n",
        "\n",
        "# 前処理とトークナイズを行う\n",
        "inputs = [preprocess_title(title)]\n",
        "batch = tokenizer.batch_encode_plus(\n",
        "    inputs, max_length=MAX_SOURCE_LENGTH, truncation=True, \n",
        "    padding=\"longest\", return_tensors=\"pt\")\n",
        "\n",
        "input_ids = batch['input_ids']\n",
        "input_mask = batch['attention_mask']\n",
        "if USE_GPU:\n",
        "    input_ids = input_ids.cuda()\n",
        "    input_mask = input_mask.cuda()\n",
        "\n",
        "# 生成処理を行う\n",
        "outputs = trained_model.generate(\n",
        "    input_ids=input_ids, attention_mask=input_mask, \n",
        "    max_length=MAX_TARGET_LENGTH,\n",
        "    # temperature=1.0,  # 生成にランダム性を入れる温度パラメータ\n",
        "    # num_beams=10,  # ビームサーチの探索幅\n",
        "    # diversity_penalty=1.0,  # 生成結果の多様性を生み出すためのペナルティパラメータ\n",
        "    # num_beam_groups=10,  # ビームサーチのグループ\n",
        "    # num_return_sequences=10,  # 生成する文の数\n",
        "    repetition_penalty=8.0,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n",
        ")\n",
        "\n",
        "# 生成されたトークン列を文字列に変換する\n",
        "generated_bodies = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                                     clean_up_tokenization_spaces=False) \n",
        "                    for ids in outputs]\n",
        "\n",
        "# 生成された文章を表示する\n",
        "for i, body in enumerate(generated_bodies):\n",
        "    print(\"\\n\".join(textwrap.wrap(f\"{i+1:2}. {body}\")))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1. スマートロックとは、スマートフォンやタブレットなどの端末に内蔵されているセキュリティ機能のこと。この「hey siri鍵開けて」を実\n",
            "現するには、androidのアプリをインストールして起動する必要がありそうだ。今回紹介するiphone向けアプリ「hey siri鍵開けて」\n",
            "は、スマホで簡単に操作できるというものだ。今回は、そんなlegoで作れるスマートロックについて紹介しよう。今回のテーマは、「siri鍵開けて\n",
            "」である。これは、googleが提供しているios向けのアプリケーションであり、ユーザーからの要望に応えるべく開発されたものだということだ。\n",
            "また、ipod touchとwi-fi接続によるパスワード入力も可能となっている。なお、snsへの投稿も可能なため、事前に登録したメールアド\n",
            "レス宛に通知を送ることも可能だ。ただし、設定画面では自動でホーム画面へ誘導されるようになっている(画面1)。画面右下の歯車マークをタップし、\n",
            "[ok]を押せば完了だ。これであとはlineアカウントの設定を行うだけでいいわけだ。ちなみにhuluでも利用できるのだが、その場合はどうすれ\n",
            "ばよいだろう。さっそく試してみることにしよう。記事執筆:河童丸\n",
            "■関連リンク・エスマックス(s-max)・エスマックス(s-max)smaxjp on twitter・info@gmail.com/sto\n",
            "re/apps/details?id=com.co.genkosha.izumihn0ck\n",
            "■関連リンク・エスマックス(s-max)・エスマックス(s-max)smaxjp on twitter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAFEYh5nkyBR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
